{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80bfae3b",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Derive the update rule and show how to train a 2-layer (1 hidden layer and 1 output layer) neural network with backpropagation for regression using the Mean Square Error loss. Assume that you are using the Sigmoid activation function for the hidden layer. Explain briefly how this is different from the update rule for the network trained for binary classification using log loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d533d",
   "metadata": {},
   "source": [
    "Let $x$ and $y$ be the input features and network output, respectively. Also, let $w$ and $b^{(1)}$, and $\\nu$ and $b^{(2)}$ be the parameters of the hidden and output layer, respectively. \n",
    "\n",
    "The hidden layer takes $x$ as input and has output \n",
    "$z_{i} = w_{ij}x_{ij} + b_{i}^{(1)}$, and $a_{i}= sigmoid(z_{i}) = \\frac{1}{1 + exp(z_{i})}$, where $i=1,...,N$ (N being the number of neurons in the hidden layer) and $j=1,...,d$ (d being the number of inputs).\n",
    "\n",
    "The output layer has $a$ as input and $\\hat{y}=\\nu_{i}a_{i} + b_{i}^{(2)}$.\n",
    "\n",
    "The loss function is given by $\\mathcal{L} = \\frac{1}{2}\\sum\\limits_{k=1}^{N}(y_{k}-\\hat{y}_{k})^{2}$\n",
    "\n",
    "We used gradient descend to update the parameters:\n",
    "\n",
    "$\\nu_{i} = \\nu_{i} - \\frac{d\\mathcal{L}}{d\\nu_{i}} = \\nu_{i} + \\alpha\\sum\\limits_{k=1}^{N}(y_{k}-\\hat{y}_{k})a_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41447af",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c745d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d46a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt(\"content/drive/MyDrive/X_train.csv\")\n",
    "Y_train = np.loadtxt(\"content/drive/MyDrive/Y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c4c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.02284419e+01  1.21199529e+02  1.44961725e+02 -5.36085172e+01\n",
      " -8.07826069e+01  7.18636506e+01  7.14660620e+01 -3.33251839e+01\n",
      "  5.97617006e+01 -1.77106285e+01 -6.57687320e+01  6.26394013e+01\n",
      "  1.47209922e+02 -1.18027722e+01  4.39625895e+01 -6.28731061e+01\n",
      "  3.44849629e+01 -5.09420832e+01 -8.49771505e+01  3.68481597e+01\n",
      "  1.11369597e+02  4.76487061e+01 -9.31874610e-02  1.55078430e+02\n",
      " -3.73116259e+01 -4.35690937e+01 -4.22120632e+01  7.20531105e+01\n",
      "  5.79452597e+01  4.78028835e+01 -1.08456030e+02 -3.73589514e+01\n",
      " -7.38892499e+01  1.19229213e+02 -1.05963551e+02  1.07139818e+02\n",
      "  7.38710742e+01  1.38870886e+02 -1.13688978e+02 -7.25512500e+01\n",
      "  1.45345079e+02 -1.67025529e+01 -2.41685991e+01 -1.47078117e+02\n",
      "  1.02947479e+02  1.36847374e+01  1.28475765e+01 -1.32796416e+02\n",
      "  1.88609830e+00  2.71429981e+01  5.15790201e+00 -9.26084192e+01\n",
      "  7.13230422e+01  4.78679217e+01  5.42322840e+01  1.29445809e+02\n",
      "  4.20499295e+01  5.62194631e+01  3.51305435e+01 -4.38039805e+01\n",
      "  1.99532266e+00 -2.30821012e+01 -6.87926522e+01  6.54517385e+01\n",
      " -5.56512156e+01  6.63166320e+01 -5.01521684e+01  2.03378441e+01\n",
      "  1.17812095e+02 -5.99427444e+01  8.40144502e+01  8.58979462e-01\n",
      " -4.92184428e+01  1.06251481e+02  1.34898497e+02 -1.15875779e+02\n",
      "  4.23338972e+01 -7.04754911e+01 -5.27518151e+01  1.60169001e+02\n",
      "  2.87183750e+01  4.51852088e+01  1.69423378e+01  4.85508301e+01\n",
      "  1.35816193e+01  6.48134269e+01  5.12569437e+01 -9.28552650e+01\n",
      " -5.16885666e+01  1.39868993e+01  1.01108398e+01 -1.12686303e+02\n",
      " -1.06145050e+02 -9.67843544e+00 -1.53967276e+01 -7.74180572e+01\n",
      "  1.59781430e+01 -8.78970533e+01 -2.21372084e+01 -6.04886263e+01]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d0d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation function\n",
    "def sigmoid(z):\n",
    "\treturn 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "  return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "# Backprop\n",
    "def backprop(W1, W2, b1, b2):\n",
    "\tfor i in range(25):\n",
    "\t\tdz2 = (a2 - y)\n",
    "\t\tdW2 = np.dot(dz2, a1.T)\n",
    "\t\tdb2 = dz2\n",
    "\t\tdz1 = np.dot(W2.T, dz2) * sigmoid(z1) * (1-sigmoid(z1))\n",
    "\t\tdW1 = np.dot(dz1, x.T)\n",
    "\t\tdb1 = dz1\n",
    "\t\tW1 = W1 - dW1\n",
    "\t\tW2 = W2 - dW2\n",
    "\t\tb1 = b1 - db1\n",
    "\t\tb2 = b2 - db2\n",
    "\t\tz1New = np.dot(W1, x) + b1\n",
    "\t\ta1New = sigmoid(z1New)\n",
    "\t\tz2New = np.dot(W2, a1New) + b2\n",
    "\t\ta2New = sigmoid(z2New)\n",
    "\t\tprint(a2New, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff767d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
