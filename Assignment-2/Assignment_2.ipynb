{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80bfae3b",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "#### Derive the update rule and show how to train a 2-layer (1 hidden layer and 1 output layer) neural network with backpropagation for regression using the Mean Square Error loss. Assume that you are using the Sigmoid activation function for the hidden layer. Explain briefly how this is different from the update rule for the network trained for binary classification using log loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d533d",
   "metadata": {},
   "source": [
    "Let $x$ and $\\hat{y}$ be the input features and network output, respectively. Also, let $w_{1}$ and $b_{1}$, and $w_{2}$ and $b^{(2)}$ be the parameters of the hidden and output layer, respectively. \n",
    "\n",
    "The hidden layer takes $x$ as input and has output \n",
    "$z_{1} = w x + b_{1}$, and $a_{1}= \\sigma(z_{1}) = \\frac{1}{1 + exp(-z_{1})}$.\n",
    "\n",
    "The output layer has $a_{1}$ as input and $\\hat{y}=w_{2}a_{1} + b_{2}$ as output.\n",
    "\n",
    "The loss function is given by $\\mathcal{L} = \\frac{1}{2N}\\sum\\limits_{k=1}^{N}(y_{k}-\\hat{y}_{k})^{2}$, where $N$ is the number of samples.\n",
    "\n",
    "We used gradient descend to update the parameters:\n",
    "\n",
    "$dw_{2} = -\\alpha\\frac{d\\mathcal{L}}{dw_{2}} = -\\alpha\\frac{d\\mathcal{L}}{d\\hat{y}}\\frac{d\\hat{y}}{dw_{2}} = \\frac{\\alpha}{N}\\sum\\limits_{k}(y_{k}-\\hat{y}_{k})a_{1} $\n",
    "\n",
    "$db_{2} = -\\alpha\\frac{d\\mathcal{L}}{db_{2}} = -\\alpha\\frac{d\\mathcal{L}}{d\\hat{y}}\\frac{d\\hat{y}}{db_{2}} = \\frac{\\alpha}{N}\\sum\\limits_{k}(y_{k}-\\hat{y}_{k}) $\n",
    "\n",
    "$dw_{1} = -\\alpha\\frac{d\\mathcal{L}}{dw_{1}} = -\\alpha\\frac{d\\mathcal{L}}{d\\hat{y}}\\frac{d\\hat{y}}{da_{1}}\\frac{da_{1}}{dz_{1}}\\frac{dz_{1}}{dw_{1}} = \\frac{\\alpha}{N}\\sum\\limits_{k}(y_{k}-\\hat{y}_{k})w_{2}a_{1}(1-a_{1})x $\n",
    "\n",
    "$db_{1} = -\\alpha\\frac{d\\mathcal{L}}{db_{1}} = -\\alpha\\frac{d\\mathcal{L}}{d\\hat{y}}\\frac{d\\hat{y}}{da_{1}}\\frac{da_{1}}{dz_{1}}\\frac{dz_{1}}{db_{1}} = \\frac{\\alpha}{N}\\sum\\limits_{k}(y_{k}-\\hat{y}_{k})w_{2}a_{1}(1-a_{1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41447af",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434fd897",
   "metadata": {},
   "source": [
    "### 1. What is the activation function that you will choose for the output layer?\n",
    "\n",
    "Since this a regression problem, i.e., the targets are continuous real values, I'll use a linear activation function. \n",
    "\n",
    "### 2. How many neurons should there be in the output layer? Why?\n",
    "\n",
    "One, since the output is only one real value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c745d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09fc3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation function\n",
    "def sigmoid(z):\n",
    "\treturn 1/(1 + np.exp(-z))\n",
    "\n",
    "#derivative of activation function\n",
    "def sigmoid_prime(z):\n",
    "  return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3c4c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define network structure \n",
    "def structure(X, Y):\n",
    "    input_size = X.shape[0] # size of input layer\n",
    "    features = X.shape[1] #number of features\n",
    "    neurons = 10 #number of neurons 10\n",
    "    output_size = Y.shape[0] # size of output layer\n",
    "    return (input_size, features, neurons, output_size)\n",
    "\n",
    "#Initialize parameters\n",
    "def par_init(input_size, neurons, output_size): \n",
    "    W1 = np.random.rand(neurons, input_size)\n",
    "    b1 = np.random.rand(neurons, 1)\n",
    "    W2 = np.random.rand(output_size, input_size)\n",
    "    b2 = np.zeros((output_size, 1))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d46a1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.loadtxt(\"content/drive/MyDrive/X_train.csv\")\n",
    "Y_train = np.loadtxt(\"content/drive/MyDrive/Y_train.csv\")\n",
    "\n",
    "print(np.shape(X_train))\n",
    "\n",
    "stru = structure(X_train,Y_train)\n",
    "\n",
    "# Define initial parameters\n",
    "W1 = np.random.rand(stru[0],stru[2])\n",
    "b1 = np.random.rand(stru[2],stru[1])\n",
    "W2 = np.random.rand(stru[2],stru[3])\n",
    "b2 = np.random.rand(stru[3],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d0d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop\n",
    "def backprop(W1, W2, b1, b2):\n",
    "\tfor i in range(25):\n",
    "\t\tdz2 = (a2 - y)\n",
    "\t\tdW2 = np.dot(dz2, a1.T)\n",
    "\t\tdb2 = dz2\n",
    "\t\tdz1 = np.dot(W2.T, dz2) * sigmoid(z1) * (1-sigmoid(z1))\n",
    "\t\tdW1 = np.dot(dz1, x.T)\n",
    "\t\tdb1 = dz1\n",
    "\t\tW1 = W1 - dW1\n",
    "\t\tW2 = W2 - dW2\n",
    "\t\tb1 = b1 - db1\n",
    "\t\tb2 = b2 - db2\n",
    "\t\tz1New = np.dot(W1, x) + b1\n",
    "\t\ta1New = sigmoid(z1New)\n",
    "\t\tz2New = np.dot(W2, a1New) + b2\n",
    "\t\ta2New = sigmoid(z2New)\n",
    "\t\tprint(a2New, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110eb093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = {\"Z1\": Z1,\"A1\": A1,\"Z2\": Z2,\"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    #number of training example\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "   \n",
    "    dZ2 = A2-Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T) \n",
    "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2,\"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2161c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, grads, learning_rate = 0.01):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "   \n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1,\"W2\": W2,\"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(X, Y, neurons, num_iterations = 1000):\n",
    "    np.random.seed(3)\n",
    "    input_unit = structure(X, Y)[0]\n",
    "    output_unit = structure(X, Y)[2]\n",
    "    \n",
    "    parameters = parameters_initialization(input_unit, neurons, output_unit)\n",
    "   \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = cross_entropy_cost(A2, Y, parameters)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = gradient_descent(parameters, grads)\n",
    "        if i % 5 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))    \n",
    "    return parameters\n",
    "\n",
    "parameters = NN(X_train, Y_train, 4, num_iterations=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
